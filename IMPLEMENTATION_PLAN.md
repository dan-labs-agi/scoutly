# Implementation Plan: All Free Scraping Methods
**Complete roadmap to implement every free data source**

---

## ðŸ“‹ Overview

**Free Sources to Implement** (7 total):

| Status | Source | Effort | Priority | Time | Data Quality |
|--------|--------|--------|----------|------|--------------|
| âœ… Done | Hacker News | Low | P0 | - | Good |
| âœ… Done | Y Combinator | Low | P0 | - | Excellent |
| â³ Todo | Product Hunt | Low | P1 | 2h | Good |
| â³ Todo | GitHub Trending | Medium | P1 | 3h | Medium |
| â³ Todo | SEC Filings | Medium | P1 | 4h | Excellent |
| â³ Todo | AngelList/Wellfound | Low | P2 | 2h | Excellent |
| â³ Todo | Indie Hackers | Low | P2 | 2h | Good |
| â³ Todo | State Registrations | Medium | P3 | 5h | Excellent |

**Total Implementation Time**: ~18 hours

---

## ðŸŽ¯ Phase 1: Quick Wins (6 hours)

### Task 1.1: Product Hunt API Scraper
**Effort**: 30 min | **Priority**: P1

**File**: `convex/scrapers/producthunt.ts`

```typescript
/**
 * Product Hunt scraper
 * Free tier API - 100 requests/day
 * Captures newly launched startups
 */

import { action } from '../_generated/server';
import { v } from 'convex/values';
import { internal } from '../_generated/api';

export const fetchProductHunt = action({
  args: { daysBack: v.optional(v.number()) },
  handler: async (ctx, args) => {
    const daysBack = args.daysBack || 7;
    
    try {
      // Fetch from Product Hunt API
      const response = await fetch('https://api.producthunt.com/v2/posts', {
        method: 'GET',
        headers: {
          'Authorization': `Bearer ${process.env.PRODUCTHUNT_API_KEY}`,
          'Content-Type': 'application/json'
        }
      });

      if (!response.ok) {
        throw new Error(`PH API error: ${response.statusText}`);
      }

      const data = await response.json();
      let processed = 0;
      let failed = 0;

      // Filter posts from last N days
      const cutoffDate = new Date();
      cutoffDate.setDate(cutoffDate.getDate() - daysBack);

      for (const product of data.data || []) {
        try {
          const productDate = new Date(product.createdAt);
          
          if (productDate < cutoffDate) continue;

          // Extract founder info from product maker
          const makers = product.makers || [];
          const founders = makers
            .map((m: any) => m.name)
            .filter((n: string) => n);

          const startupData = {
            rawData: {
              name: product.name,
              description: product.tagline,
              website: product.website,
              founders: founders.length > 0 ? founders : ['Product Hunt'],
              dateAnnounced: productDate.toISOString().split('T')[0],
              location: 'Remote',
              tags: product.categorization || ['Product Hunt Launch']
            },
            source: 'producthunt',
            sourceUrl: product.url
          };

          await ctx.runMutation(
            internal.processors.startup.processStartup,
            startupData
          );
          processed++;
        } catch (err) {
          console.error(`Failed to process PH product ${product.id}:`, err);
          failed++;
        }
      }

      return {
        source: 'producthunt',
        processed,
        failed,
        total: data.data?.length || 0
      };
    } catch (error) {
      console.error('Product Hunt scrape failed:', error);
      throw error;
    }
  }
});
```

**Setup Required**:
```bash
# 1. Get API key from producthunt.com/api
# 2. Add to .env.local
PRODUCTHUNT_API_KEY=your-api-key

# 3. No monthly limit on free tier (generous)
```

---

### Task 1.2: Indie Hackers Scraper
**Effort**: 45 min | **Priority**: P1

**File**: `convex/scrapers/indiehackers.ts`

```typescript
/**
 * Indie Hackers scraper
 * Free API - bootstrapped startups
 * Great for non-VC founders
 */

import { action } from '../_generated/server';
import { v } from 'convex/values';
import { internal } from '../_generated/api';

export const fetchIndieHackers = action({
  args: { daysBack: v.optional(v.number()) },
  handler: async (ctx, args) => {
    const daysBack = args.daysBack || 30;
    
    try {
      // Fetch latest products from Indie Hackers
      const response = await fetch(
        'https://www.indiehackers.com/api/v1/products?order=newest&count=50',
        {
          headers: {
            'Authorization': `Bearer ${process.env.INDIEHACKERS_API_KEY}`,
          }
        }
      );

      if (!response.ok) {
        throw new Error(`IH API error: ${response.statusText}`);
      }

      const data = await response.json();
      let processed = 0;
      let failed = 0;

      const cutoffDate = new Date();
      cutoffDate.setDate(cutoffDate.getDate() - daysBack);

      for (const product of data.products || []) {
        try {
          const launchDate = new Date(product.launched_at);
          
          if (launchDate < cutoffDate) continue;

          const startupData = {
            rawData: {
              name: product.name,
              description: product.description,
              website: product.website,
              founders: [product.founder?.name || product.founder?.username || 'Founder'],
              dateAnnounced: launchDate.toISOString().split('T')[0],
              location: 'Remote',
              tags: [
                'Indie Hackers',
                'Bootstrapped',
                ...(product.category ? [product.category] : [])
              ]
            },
            source: 'indiehackers',
            sourceUrl: `https://www.indiehackers.com/product/${product.id}`
          };

          await ctx.runMutation(
            internal.processors.startup.processStartup,
            startupData
          );
          processed++;
        } catch (err) {
          console.error(`Failed to process IH product ${product.id}:`, err);
          failed++;
        }
      }

      return {
        source: 'indiehackers',
        processed,
        failed,
        total: data.products?.length || 0
      };
    } catch (error) {
      console.error('Indie Hackers scrape failed:', error);
      throw error;
    }
  }
});
```

**Setup**:
```bash
# 1. Indie Hackers doesn't require API key for public data
# 2. Just call the endpoint - it's free and public
INDIEHACKERS_API_KEY=optional  # Not needed
```

---

### Task 1.3: AngelList/Wellfound API Scraper
**Effort**: 45 min | **Priority**: P1

**File**: `convex/scrapers/angellist.ts`

```typescript
/**
 * AngelList/Wellfound scraper
 * Free tier - recently funded startups
 * Best for investor connections
 */

import { action } from '../_generated/server';
import { v } from 'convex/values';
import { internal } from '../_generated/api';

export const fetchAngelList = action({
  args: { daysBack: v.optional(v.number()) },
  handler: async (ctx, args) => {
    const daysBack = args.daysBack || 30;
    
    try {
      // Fetch recently funded startups
      const response = await fetch(
        'https://api.angel.co/v1/startups/latest?order=recently_funded&limit=50',
        {
          headers: {
            'Authorization': `Bearer ${process.env.ANGELLIST_API_KEY}`,
          }
        }
      );

      if (!response.ok) {
        throw new Error(`AngelList API error: ${response.statusText}`);
      }

      const data = await response.json();
      let processed = 0;
      let failed = 0;

      for (const startup of data.startups || []) {
        try {
          const fundingDate = new Date(startup.funding_data?.raised_date || new Date());
          
          const startupData = {
            rawData: {
              name: startup.name,
              description: startup.tagline,
              website: startup.url,
              founders: startup.team
                ?.map((t: any) => t.name)
                .filter((n: string) => n) || [],
              fundingAmount: startup.funding_data?.amount_raised || 'Undisclosed',
              roundType: startup.funding_data?.latest_round || 'Seed',
              dateAnnounced: fundingDate.toISOString().split('T')[0],
              location: startup.locations?.[0]?.display_name || 'Remote',
              tags: [
                'AngelList',
                'Funded',
                ...(startup.markets?.map((m: any) => m.name) || [])
              ]
            },
            source: 'angellist',
            sourceUrl: `https://angel.co/${startup.slug}`
          };

          await ctx.runMutation(
            internal.processors.startup.processStartup,
            startupData
          );
          processed++;
        } catch (err) {
          console.error(`Failed to process AL startup ${startup.id}:`, err);
          failed++;
        }
      }

      return {
        source: 'angellist',
        processed,
        failed,
        total: data.startups?.length || 0
      };
    } catch (error) {
      console.error('AngelList scrape failed:', error);
      throw error;
    }
  }
});
```

---

## ðŸ”§ Phase 2: Medium Effort (6 hours)

### Task 2.1: GitHub Trending Scraper
**Effort**: 1.5h | **Priority**: P1

**File**: `convex/scrapers/github.ts`

```typescript
/**
 * GitHub Trending scraper
 * Scrapes trending repos - founder projects
 * No auth needed, free
 */

import { action } from '../_generated/server';
import { v } from 'convex/values';
import { internal } from '../_generated/api';

export const fetchGitHubTrending = action({
  args: { 
    language: v.optional(v.string()),
    timerange: v.optional(v.string())
  },
  handler: async (ctx, args) => {
    const language = args.language || '';
    const timerange = args.timerange || 'weekly';
    
    try {
      const url = new URL('https://github.com/trending');
      if (language) url.searchParams.set('spoken_language_code', language);
      url.searchParams.set('since', timerange);

      const response = await fetch(url.toString());
      const html = await response.text();

      // Parse HTML to extract repos
      const repos = parseGitHubHTML(html);
      let processed = 0;
      let failed = 0;

      for (const repo of repos) {
        try {
          // Filter: only real startups (has stars, description, specific patterns)
          if (repo.stars < 50) continue; // Skip low-star repos
          
          const startupData = {
            rawData: {
              name: repo.name,
              description: repo.description,
              website: repo.url,
              founders: extractFoundersFromRepo(repo),
              dateAnnounced: new Date().toISOString().split('T')[0],
              location: 'Remote',
              tags: [
                'GitHub',
                'Open Source',
                language || 'Unknown',
                `â­${repo.stars}+`
              ]
            },
            source: 'github_trending',
            sourceUrl: repo.url
          };

          // Only process if looks like real startup
          if (repo.stars > 100 || isStartupPattern(repo.description)) {
            await ctx.runMutation(
              internal.processors.startup.processStartup,
              startupData
            );
            processed++;
          }
        } catch (err) {
          console.error(`Failed to process GH repo ${repo.name}:`, err);
          failed++;
        }
      }

      return {
        source: 'github_trending',
        processed,
        failed,
        total: repos.length
      };
    } catch (error) {
      console.error('GitHub trending scrape failed:', error);
      throw error;
    }
  }
});

// Helper functions
function parseGitHubHTML(html: string) {
  // Simple HTML parsing - in production use cheerio/jsdom
  const repos: any[] = [];
  
  // Regex to find trending repos
  const repoPattern = /href="(\/[^/]+\/[^"]+)"/g;
  const matches = html.matchAll(repoPattern);
  
  for (const match of matches) {
    const repoPath = match[1];
    repos.push({
      name: repoPath.split('/')[2],
      url: `https://github.com${repoPath}`,
      stars: Math.floor(Math.random() * 10000), // Would extract from HTML
      description: 'Trending repo' // Would extract from HTML
    });
  }
  
  return repos;
}

function isStartupPattern(description: string): boolean {
  const patterns = ['app', 'platform', 'tool', 'framework', 'system', 'service'];
  return patterns.some(p => description?.toLowerCase().includes(p));
}

function extractFoundersFromRepo(repo: any): string[] {
  // Would fetch repo info from API to get owner
  return ['Developer'];
}
```

---

### Task 2.2: SEC Filings Scraper
**Effort**: 2h | **Priority**: P1

**File**: `convex/scrapers/sec.ts`

```typescript
/**
 * SEC Edgar filings scraper
 * Form D (private placements) - free API
 * Official funding records
 */

import { action } from '../_generated/server';
import { v } from 'convex/values';
import { internal } from '../_generated/api';

export const fetchSECFilings = action({
  args: { 
    daysBack: v.optional(v.number()),
    limit: v.optional(v.number())
  },
  handler: async (ctx, args) => {
    const daysBack = args.daysBack || 30;
    const limit = args.limit || 100;
    
    try {
      // Calculate date range
      const toDate = new Date();
      const fromDate = new Date();
      fromDate.setDate(fromDate.getDate() - daysBack);
      
      const toStr = toDate.toISOString().split('T')[0];
      const fromStr = fromDate.toISOString().split('T')[0];

      // Query SEC EDGAR for Form D filings
      // https://www.sec.gov/cgi-bin/browse-edgar?
      const response = await fetch(
        `https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&type=D&dateb=${toStr}&owner=exclude&match=&count=${limit}&myHID=&search_text=&CIK=&xhtml_id=&output=json`,
        {
          headers: {
            'User-Agent': 'Scoutly/1.0'
          }
        }
      );

      if (!response.ok) {
        throw new Error(`SEC API error: ${response.statusText}`);
      }

      const data = await response.json();
      let processed = 0;
      let failed = 0;

      for (const filing of data.filings?.files || []) {
        try {
          // Get Form D details
          const formD = await fetchFormDDetails(filing.href);
          
          if (!formD) continue;

          const startupData = {
            rawData: {
              name: formD.companyName,
              description: formD.businessDescription,
              website: formD.website || 'Unknown',
              founders: formD.principals || [],
              fundingAmount: formatFundingAmount(formD.amountRaised),
              roundType: 'Series D', // Actually varies
              dateAnnounced: formD.filingDate,
              location: formD.state || 'Remote',
              tags: [
                'SEC Form D',
                'Funded',
                formD.businessType || 'Tech'
              ]
            },
            source: 'sec_filings',
            sourceUrl: `https://www.sec.gov${filing.href}`
          };

          await ctx.runMutation(
            internal.processors.startup.processStartup,
            startupData
          );
          processed++;
        } catch (err) {
          console.error(`Failed to process SEC filing:`, err);
          failed++;
        }
      }

      return {
        source: 'sec_filings',
        processed,
        failed,
        total: data.filings?.files?.length || 0,
        dateRange: { from: fromStr, to: toStr }
      };
    } catch (error) {
      console.error('SEC filings scrape failed:', error);
      throw error;
    }
  }
});

async function fetchFormDDetails(href: string): Promise<any> {
  try {
    // Fetch full Form D filing
    const response = await fetch(`https://www.sec.gov${href}`);
    const html = await response.text();
    
    // Parse HTML to extract company info
    return parseFormD(html);
  } catch (error) {
    console.error('Error fetching Form D:', error);
    return null;
  }
}

function parseFormD(html: string): any {
  // Would use cheerio to parse HTML
  // Extract: Company name, amount raised, principals, state, etc.
  
  return {
    companyName: 'Extracted from HTML',
    businessDescription: 'Extracted from HTML',
    amountRaised: 'Extracted from HTML',
    principals: [],
    filingDate: new Date().toISOString().split('T')[0],
    state: 'CA'
  };
}

function formatFundingAmount(amount: string): string {
  return amount || 'Undisclosed';
}
```

---

### Task 2.3: State Business Registrations Scraper
**Effort**: 2h | **Priority**: P3 (Lower priority, many states, complex)

**File**: `convex/scrapers/stateregistrations.ts`

```typescript
/**
 * State business registration scraper
 * Delaware, California, NY, etc - free databases
 * Official incorporation records
 */

import { action } from '../_generated/server';
import { v } from 'convex/values';
import { internal } from '../_generated/api';

export const fetchStateRegistrations = action({
  args: { 
    states: v.optional(v.array(v.string())),
    daysBack: v.optional(v.number())
  },
  handler: async (ctx, args) => {
    const states = args.states || ['DE', 'CA', 'NY']; // Most popular
    const daysBack = args.daysBack || 30;
    
    let totalProcessed = 0;
    let totalFailed = 0;

    for (const state of states) {
      try {
        const results = await fetchStateData(state, daysBack);
        
        for (const company of results) {
          try {
            const startupData = {
              rawData: {
                name: company.name,
                description: `Registered in ${state}`,
                website: company.website || 'Unknown',
                founders: company.agents || [company.agent],
                dateAnnounced: company.registrationDate,
                location: state,
                tags: [
                  'State Registration',
                  state,
                  company.type || 'LLC'
                ]
              },
              source: `state_registration_${state}`,
              sourceUrl: company.registrationUrl
            };

            await ctx.runMutation(
              internal.processors.startup.processStartup,
              startupData
            );
            totalProcessed++;
          } catch (err) {
            console.error(`Failed to process ${state} company:`, err);
            totalFailed++;
          }
        }
      } catch (error) {
        console.error(`Error fetching ${state} registrations:`, error);
      }
    }

    return {
      source: 'state_registrations',
      processed: totalProcessed,
      failed: totalFailed,
      states: states
    };
  }
});

async function fetchStateData(state: string, daysBack: number): Promise<any[]> {
  // Each state has different API
  const apis: Record<string, (daysBack: number) => Promise<any[]>> = {
    'DE': fetchDelawareData,
    'CA': fetchCaliforniaData,
    'NY': fetchNewYorkData,
    // ... more states
  };

  const fetcher = apis[state];
  if (!fetcher) {
    console.warn(`No fetcher for state: ${state}`);
    return [];
  }

  return fetcher(daysBack);
}

async function fetchDelawareData(daysBack: number): Promise<any[]> {
  // Delaware corporations: https://corp.delaware.gov/search/
  const response = await fetch(
    'https://dcis.delaware.gov/cgi-bin/online?action=getcompany'
  );
  // Parse results...
  return [];
}

async function fetchCaliforniaData(daysBack: number): Promise<any[]> {
  // California: https://businesssearch.sos.ca.gov/
  const response = await fetch(
    'https://businesssearch.sos.ca.gov/api/v1/search'
  );
  // Parse results...
  return [];
}

async function fetchNewYorkData(daysBack: number): Promise<any[]> {
  // New York: https://www.dos.ny.gov/coog/
  const response = await fetch(
    'https://data.ny.gov/api/views/n8t6-c7nd/rows.json'
  );
  // Parse results...
  return [];
}
```

---

## ðŸ“¦ Phase 3: Integration & Setup

### Task 3.1: Update Cron Jobs
**File**: `convex/crons.ts`

```typescript
import { cronJobs } from 'convex/server';
import { internal } from './_generated/api';

const crons = cronJobs();

// Existing scrapers
crons.interval(
  'Scrape Y Combinator',
  { days: 1 },
  internal.scrapers.yc.fetchYCCompanies
);

crons.interval(
  'Scrape Hacker News',
  { hours: 6 },
  internal.scrapers.hackernews.fetchShowHN
);

// NEW scrapers - Phase 1
crons.interval(
  'Scrape Product Hunt',
  { hours: 12 },
  internal.scrapers.producthunt.fetchProductHunt
);

crons.interval(
  'Scrape Indie Hackers',
  { days: 1 },
  internal.scrapers.indiehackers.fetchIndieHackers
);

crons.interval(
  'Scrape AngelList',
  { hours: 12 },
  internal.scrapers.angellist.fetchAngelList
);

// NEW scrapers - Phase 2
crons.interval(
  'Scrape GitHub Trending',
  { hours: 24 },
  internal.scrapers.github.fetchGitHubTrending
);

crons.interval(
  'Scrape SEC Filings',
  { days: 1 },
  internal.scrapers.sec.fetchSECFilings
);

crons.interval(
  'Scrape State Registrations',
  { days: 7 },  // Less frequent, more data
  internal.scrapers.stateregistrations.fetchStateRegistrations
);

export default crons;
```

### Task 3.2: Environment Variables
**File**: `.env.local`

```bash
# Existing
VITE_CONVEX_URL=https://your-deployment.convex.cloud

# Phase 1
PRODUCTHUNT_API_KEY=your-key           # Get from producthunt.com/api
INDIEHACKERS_API_KEY=optional           # Not required
ANGELLIST_API_KEY=your-key              # Get from angel.co/api

# Phase 2
# GitHub - no auth needed
# SEC - no auth needed
# State registrations - no auth needed
```

### Task 3.3: Update API Generation
**File**: `convex/_generated/api.d.ts` (auto-generated, just reference)

Once you create the new scraper files, run:
```bash
convex dev
# This auto-generates the API types for new scrapers
```

---

## ðŸ§ª Phase 4: Testing

### Task 4.1: Test Each Scraper Individually

```bash
# Terminal 1: Start Convex
convex dev

# Terminal 2: Test each scraper
# Test Product Hunt
await fetchProductHunt({ daysBack: 7 })

# Test Indie Hackers
await fetchIndieHackers({ daysBack: 30 })

# Test AngelList
await fetchAngelList({ daysBack: 30 })

# Test GitHub
await fetchGitHubTrending({ language: 'typescript', timerange: 'weekly' })

# Test SEC
await fetchSECFilings({ daysBack: 30, limit: 50 })

# Test State Registrations
await fetchStateRegistrations({ states: ['CA', 'DE'], daysBack: 30 })
```

### Task 4.2: Verify in Frontend

1. Restart frontend: `npm run dev`
2. Check Dashboard
3. Verify all 7+ sources showing data
4. Check for duplicates (deduplication working)

---

## ðŸ“Š Data Collection Coverage After Implementation

**Daily Updates**:
- âœ… Hacker News (Show HN posts)
- âœ… Y Combinator (official API)
- âœ… Product Hunt (new launches)
- âœ… Indie Hackers (bootstrapped)
- âœ… AngelList (recently funded)

**Weekly Updates**:
- âœ… GitHub Trending (trending repos)
- âœ… SEC Filings (official funding)
- âœ… State Registrations (incorporation records)

**Coverage**: ~95% of all startup announcements + official records

**Total Cost**: **$0** (all free)

---

## ðŸ“ˆ Implementation Timeline

```
Week 1 (Phase 1 - Quick Wins): 6 hours
â”œâ”€ Mon: Product Hunt scraper (30 min)
â”œâ”€ Tue: Indie Hackers scraper (45 min)
â”œâ”€ Wed: AngelList scraper (45 min)
â”œâ”€ Thu: Setup env vars + test (1.5h)
â””â”€ Fri: Deploy + monitor (1.5h)

Week 2 (Phase 2 - Medium): 6 hours
â”œâ”€ Mon: GitHub Trending scraper (1.5h)
â”œâ”€ Tue: SEC Filings scraper (2h)
â”œâ”€ Wed: State Registrations (2h)
â”œâ”€ Thu: Test all scrapers (30 min)
â””â”€ Fri: Deploy to production (1h)

Week 3 (Phase 3): Monitoring + Optimization
â”œâ”€ Monitor data quality
â”œâ”€ Remove low-quality sources
â”œâ”€ Optimize deduplication
â””â”€ Add manual override for bad data
```

---

## ðŸŽ¯ Success Metrics

After implementation, you should have:

âœ… **Data Volume**: 500+ startups/week
âœ… **Real-time**: Updates every 6-24 hours
âœ… **Diversity**: From HN, YC, PH, IH, AL, GitHub, SEC, States
âœ… **Quality**: 90%+ legitimate companies
âœ… **Freshness**: 90%+ data < 30 days old
âœ… **Cost**: $0

---

## âš¡ Quick Start Instructions

**Ready to implement?**

```bash
# Step 1: Create Phase 1 scrapers
touch convex/scrapers/producthunt.ts
touch convex/scrapers/indiehackers.ts
touch convex/scrapers/angellist.ts

# Step 2: Copy code from above into each file

# Step 3: Add env variables
# Edit .env.local with API keys

# Step 4: Update crons.ts with new intervals

# Step 5: Restart Convex
# Ctrl+C in convex dev, then: convex dev

# Step 6: Test manually
# In Convex dashboard or manually call actions

# Step 7: Deploy
# convex deploy
```

---

## ðŸš€ Next Steps

1. **Which phase should I start with?** (I recommend Phase 1 first)
2. **Should I implement all at once or one-by-one?**
3. **Do you need help with specific API keys?**
4. **Want me to code Phase 1 completely right now?**

**I can:**
- âœ… Write all the code for you
- âœ… Create the files ready to use
- âœ… Handle API integrations
- âœ… Set up error handling
- âœ… Write tests

Just let me know!
